<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A fine-tuning framework to improve LLMs' strategic exploration and sequential decision making capabilities">
  <meta name="keywords" content="LLM, Synthetic data, Multiturn finetuning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>paprika</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation" style="background-color: #333; color: #fff;">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" style="color: #fff;">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#abstract" style="color: #fff; border-bottom: 0px solid #fff;">
        Abstract
      </a>
      <a class="navbar-item" href="#setup" style="color: #fff; border-bottom: 0px solid #fff;">
        PAPRIKA
      </a>
      <a class="navbar-item" href="#paprika" style="color: #fff; border-bottom: 0px solid #fff;">
        Empirical Results
      </a>
      <a class="navbar-item" href="#BibTeX" style="color: #fff; border-bottom: 0px solid #fff;">
        BibTeX
      </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Training a Generally Curious Agent</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tajwarfahim.github.io/">Fahim Tajwar</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://yidingjiang.github.io/">Yiding Jiang</a><sup>*1</sup>,</span>
              <span class="author-block">
              <a href="https://abitha-thankaraj.github.io/">Abitha Thankaraj</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/sumaitasr/">Sumaita Sadia Rahman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://zicokolter.com/">J Zico Kolter</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~schneide/">Jeff Schneider</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a><sup>1</sup>,
            </span>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
            <span class="author-block"><sup>1</sup>Carnegie Mellon University</span>
            <span class="author-block"><sup>2</sup>North Carolina State University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.17543"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tajwarfahim/paprika"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ftajwar/paprika_SFT_dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>SFT Dataset</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ftajwar/paprika_preference_dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Preference Dataset</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/ftajwar/paprika_Meta-Llama-3.1-8B-Instruct"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-cogs"></i>
                  </span>
                  <span>Model</span>
                  </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>

</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/final_picture_teaser.png"
        alt="Teaser image."
        class="teaser-image"/>
      <h2 class="subtitle has-text-centered">
        Overview of <b>PAPRIKA</b>. We design a diverse set of tasks where an LLM agent needs strategic information gathering to succeed, 
        then train an LLM on self-generated data to prefer higher performing trajectories. The resulting behavior learned by PAPRIKA 
        can transfer zero-shot to unseen tasks, showcasing its potential to build general decision making agents.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="abstract">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Efficient exploration is essential for intelligent systems interacting with their environment, but 
            existing language models often fall short in scenarios that require strategic information gathering. 
            In this paper, we present <b>PAPRIKA</b>, a fine-tuning approach that enables language models to develop general 
            decision-making capabilities that are not confined to particular environments. By training on synthetic 
            interaction data from different tasks that require diverse strategies, <b>PAPRIKA</b> teaches models to explore and 
            adapt their behavior on a new task based on environment feedback in-context without more gradient updates. 
            Experimental results show that models fine-tuned with <b>PAPRIKA</b> can effectively transfer their learned 
            decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, 
            our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. 
            To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories 
            from tasks with high learning potential. These results suggest a promising path towards AI systems that can 
            autonomously solve novel sequential decision-making problems that require interactions with the external world.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  <hr>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h2 class="title is-3" id="setup">Problem Setting</h2>
        <div class="content has-text-justified">
          <p>
            The goal of our paper is to develop a scalable method to instill better strategic exploration and sequential decision-making capabilities into LLMs. 
            Prior works (<a href="https://arxiv.org/abs/2310.07064">Krishnamurthy et al., 2024</a>) have shown that LLMs can perform poorly on even the simple decision making task of multi-armed bandits. 
            <a href="https://arxiv.org/abs/2410.06238">Nie et al., 2024</a> has since then demonstrated that LLMs can be taught to perform better on bandits after fine-tuning them on synthetic trajectories 
            generated by known algorithms such as UCB. However, this idea is limited in scope for three reasons:
          </p>
          <ul>
            <li><b>(1)</b> We want LLMs to perform strategic exploration and decision making in more complex settings</li>
            <li><b>(2)</b> For most tasks, there is no known algorithm like UCB to generate good synthetic trajectories from</li>
            <li><b>(3)</b> It can be infeasible to collect data for all tasks that we care about</li>
          </ul>
        </div>

        <h2 class="title is-3" id="paprika">PAPRIKA</h2>
        <div class="content has-text-justified">
          PAPRIKA aims to solve the above problems. First, we design a suite of complex decision-making tasks that require strategic information gathering to succeed. Next, we show that in the absence of known good algorithms, existing LLMs can generate trajectories with better decision making behaviors through diversity-encouraging sampling. We then finetune the LLMs to prefer higher performing trajectories (in a fashion similar to <a href="https://arxiv.org/abs/2203.14465">STaR</a>) and show that this leads to better decision making abilities at test-time. More importantly, <b>these behaviors often generalize to unseen task groups without additional training</b>. Finally, we propose a general curriculum learning algorithm that can dynamically choose which subset of tasks to train on next to improve data efficiency of such training methods. We next describe each component of PAPRIKA.
        </div>
        
        <h3 class="title is-4">Task Design</h3>
        <div class="content has-text-justified">
          <!-- Create Bullet Point List to Summarize -->
          To both evaluate and then train LLMs, we design 10 diverse textual task groups, each of which comprises of partially observable tasks that require multiturn interaction with the task environment, strategic exploration and good sequential decision making abilities for an agent to succeed at them. Below is a summary of these task groups.
        </div>

        <center>
          <img src="./static/images/paprika_task_groups.png"
            width="90%"
            alt="paprika_task_groups"
            class="paprika_task_groups"/>
        </center>

        <h3 class="title is-4">Optimization</h3>
        <div class="content has-text-justified">
          <!-- Create Bullet Point List to Summarize -->
          We use a multiturn variant of supervised fine-tuning (SFT) and Direct Preference Optimization (<a href="https://arxiv.org/abs/2305.18290">DPO</a>) to fine-tune our models, where log probabilities are calculated autoregressively over the entire trajectory, but then
          only the log probabilities of the agent generated tokens are considered for calculating the training loss. In practice, we first run supervised finetuning, and then use the sum of SFT and DPO (similar to <a href="https://arxiv.org/abs/2404.19733">RPO</a>) losses to optimize our LLMs.
        </div>

        <center>
          <img src="./static/images/optimization_objective.png"
            width="40%"
            alt="optimization_objective"
            class="optimization_objective"/>
        </center>

        <h3 class="title is-4">Scalable Online Curriculum Learning Algorithm</h3>
        <div class="content has-text-justified">
          PAPRIKA's primary bottleneck lies in generating training trajectories, rather than model updates. So it is crucial that we generate more rollouts on tasks that have high learning potential. 
        </div>

        <div class="content has-text-justified">
          However, it is hard to know which tasks would have high learning potential without generating rollouts first. We make the additional assumption that similar tasks have similar learning potential, and that we have task similarity groups over our set of tasks. Under this assumption, we can sample a few tasks from a task group, generate multiple trajectories per task to calculate the coefficient of variation (over number of turns, which we use as a proxy for reward). We use the coefficient of variation as a metric for measuring learning potential.
        </div>

        <center>
          <img src="./static/images/coefficient_of_variation.png"
            width="50%"
            alt="learning_potential"
            class="learning_potential"/>
        </center>

        <div class="content has-text-justified">
          Once we have metric for measuring learning potential, we naturally want to maximize the learning potential of our sampled dataset, and we can treat this as multi-armed bandit (MAB) problem, for which we can employ the Upper Confidence Bound (UCB) algorithm to sample tasks from the collection of task groups. 
        </div>

        <center>
          <img src="./static/images/task_selection_with_UCB.png"
            width="50%"
            alt="curriculum_algorithm"
            class="curriculum_algorithm"/>
        </center>
        

        <h2 class="title is-3" id="empirical">Empirical Results</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">PAPRIKA improves LLM decision making abilties.</h3>
        First, training on 10 diverse tasks groups result in performance improvement on heldout tasks from each group.

        <img src="./static/images/paprika_success_rate.png"
          alt="paprika_success_rate"
          width="75%"
          class="paprika_success_rate"/>
        </center>

        <div class="content has-text-centered">
          <b>(PAPRIKA improves success rate on a diverse range of task groups)</b> Average success rate on 6 representative task groups, with shaded areas representing standard error over 3 random seeds. PAPRIKA improves performance on all of them after fine-tuning on only roughly 22,500 total trajectories.
        </div>

        We characterize fine-tuning methods based on three key properties:
        <div class="content has-text-justified">
          <ol>
            <li><b>on-policy sampling:</b> explicit sampling of new responses from the policy or purely learning from offline data</li>
            <li><b>on-policy sample reuse:</b> for only those approaches that perform on-policy sampling, whether the approach makes more 
              than one gradient update on a given prompt-response (ùë•, ùë¶) pair</li>
            <li><b>negative gradient:</b> whether the approach explicitly minimizes a loss that attempts to ‚Äúpush-down‚Äù likelihood on 
              certain responses by multiplying the gradient of their likelihood with a negative coefficient </li>
          </ol>
        </div>
        <center>
        <img src="./static/images/characterize.png"
          alt="On-Policy Sampling."
          width="75%"
          class="on-policy-image"/>
        </center>
        <div class="content has-text-centered">
          We characterize existing fine-tuning methods using these criterion.
        </div>
        <!-- Interpolating. -->
        <h3 class="title is-4">On-Policy Sampling</h3>
        <div class="content has-text-justified">
          <p>
            To study the impact of on-policy sampling, we vary the extent to which updates are made on data from the current policy. 
            Below, we introduce Algorithm 1, which is a encapsulates how fine-tuning algorithms updates on preference data.
          </p>
        </div>
        <center>
        <img src="./static/images/finetuning_algo.png"
          alt="Algorithm 1."
          width="75%"
          class="algo-one-image"/>
        </center>
        <div class="content has-text-centered">
          We characterize existing fine-tuning methods using the following algorithmic formulation.
        </div>

        <div class="content has-text-justified">
          <p>
            One way in which we can control the amount of on-policy sampling in Algorithm 1 is by by varying the total number of samples |ùíü| = ùêµ/ùê∂ √ó ùê∂ = ùêµ 
            used for a given training iteration but keeping minibatch size ùëÄ used for the gradient update fixed, assuming the algorithm performs exactly one pass over all this sampled data. One result from the paper is shown below:
          </p>
        </div>

        <center>
        <img src="./static/images/alpaca_bs.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        <div class="content has-text-centered">
          <p>
            <b>Performance on AlpacaFarm with varying batch sizes.</b> We see that decreasing the batch size improves performance, 
            which leads to more on-policy sampling leads to better performance on the AlpacaFarm domain. Empirically, we see benefits 
            of this sampling when the reference policy is far from the optimal policy.
          </p>
        </div>
        
        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Takeaway: </b>On-policy sampling generally improves performance and efficiency, especially in cases when the 
              peak of reward appears farther from the reference policy, even when the reward model is learned from the same 
              preference dataset that methods use without on-policy learning also use. In some cases, sample reuse can reduce 
              the dependency on on-policy sampling of data, but it presents a tradeoff by reducing exploration of the response space.
            </p>
          </div>
        </div>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Negative Gradients</h3>
        <div class="content has-text-justified">
          <p>
          To understand the role of negative gradient, we will compare contrastive algorithms (e.g., DPO or IPO) with maximum likelihood supervised methods (e.g., Best-of-N) in a fully offline setting, 
          where no new on-policy samples are collected. In the paper, we compare various algorithms and also explore mechanisms behind the behaviors of methods. 

          Our main finding is that negative gradients often speed up convergence of algorithms, often allowing them to reach a better solution. For example, 
          in one of the settings, we compare supervised Best-of-N to supervised Best-of-N with a negative gradient explicitly added and observe that the latter
          is able to quickly learn to find a better solution. Likewise we also find DPO attains better results.
          </p>
        </div>

        <center>
        <img src="./static/images/neg_grad.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        
        <div class="content has-text-centered">
          <p>
            <b>Negative gradients in AlpacaFarm (left) and UltraFeedback (right).</b> For these domains, we consider the increase in average gold reward 
            compared to the SFT model for different offline approaches. Algorithms with a negative gradient such as DPO outperform approaches 
            such as Pref-FT, which do not utilize any negative gradient term in their objective.
          </p>
        </div>

        <center>
        <img src="./static/images/neg_grad_rew.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        <div class="content has-text-centered">
          <p>
            <b>DPO vs Pref-FT Reward Margin.</b> The reward margin between the preferred and dispreferred responses for contrastive algorithms such as 
            DPO is higher than maximum likelihood algorithms such as Pref-FT.
          </p>
        </div>
        
        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Takeaway: </b> A negative gradient improves over offline supervised methods when the peak in the reward appears in less 
              likely regions of the reference policy. It can increase the likelihood of the preferred response, when the dispreferred response is sufficiently 
              different from the prefered response, model capacity is large, and the reference initialization is chosen appropriately. 
              If not, the margin between log likelihoods of preferred and dispreferred will still be larger when a negative gradient is used, 
              but the recovered probability mass will go into increasing likelihoods of other responses, not the preferred response necessarily.
            </p>
          </div>
        </div>

        <h3 class="title is-4">Complementary Nature of On-Policy Sampling and Negative Gradients</h3>
        <div class="content has-text-justified">
          <p>
            On-policy sampling and offline negative gradients present complementary benefits, in that
            the best offline loss function with negative gradients can be used to train on on-policy data,
            improving over on-policy RL or supervised learning. Conceptually, while sampling responses
            on-policy provides coverage of the response space, an effective negative gradient loss provides
            a stronger learning signal given a set of samples. It can also result in computational benefits.
          </p>
        </div>

        <center>
        <img src="./static/images/complimentarity.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        
        <div class="content has-text-centered">
          <p>
            <b>Complementary benefit of on-policy sampling and negative gradients on the synthetic LLM length experiments.</b> 
            Online DPO performs the best where optimal policy and reference policy lies far from each other 
            (min length and skew length), and all algorithms perform similarly when these two policies are close (mode length).
          </p>
        </div>
        <!--/ Re-rendering. -->

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Takeaway: </b>We observe that the best offline loss function with negative gradients can be used to train on on-policy data, improving over on-policy RL or supervised learning. 
              In our experiments, we find this loss function to be a contrastive loss akin to DPO or IPO. Running these contrastive methods on on-policy data helps improve performance. 
              This can also result in computational benefits, improving over wall-clock time.
            </p>
          </div>
        </div>
        
        <hr>
        
      </div>
    </div>
    <div class="content has-text-centered">
      <p>
        <b>Empirical Example contrasting mode-seeking (Reverse KL) and mode-covering (forward KL) objectives.</b>
      </p>
    </div>
    <div class="box" style="background-color:aliceblue">
      <div class="content has-text-justified">
        <p>
          <b>Takeaway: </b>We conceptually unify on-policy sampling and negative gradients under the notion of mode-seeking objectives, 
          extended to categorical distributions. An analysis of the behavior of some representative mode-seeking and mode-covering objectives
          corroborates our empirical observations in this paper.
        </p>
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>

<hr>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> 
      @misc{tajwar2025traininggenerallycuriousagent,
        title={Training a Generally Curious Agent}, 
        author={Fahim Tajwar and Yiding Jiang and Abitha Thankaraj and Sumaita Sadia Rahman and J Zico Kolter and Jeff Schneider and Ruslan Salakhutdinov},
        year={2025},
        eprint={2502.17543},
        archivePrefix={arXiv},
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2502.17543}, 
      }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2502.17543">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/tajwarfahim/paprika" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <p>
        Corresponding Author: <a href="mailto:ftajwar@cs.cmu.edu">Fahim Tajwar</a>.<br>
        We thank the Nerfies Team for their <a href="https://github.com/nerfies/nerfies.github.io">website template</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
