<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A fine-tuning framework to improve LLMs' strategic exploration and sequential decision making capabilities">
  <meta name="keywords" content="LLM, Synthetic data, Multiturn finetuning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>paprika</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation" style="background-color: #333; color: #fff;">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" style="color: #fff;">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#abstract" style="color: #fff; border-bottom: 0px solid #fff;">
        Abstract
      </a>
      <a class="navbar-item" href="#setup" style="color: #fff; border-bottom: 0px solid #fff;">
        Setup
      </a>
      <a class="navbar-item" href="#empirical" style="color: #fff; border-bottom: 0px solid #fff;">
        Empirical Analysis
      </a>
      <a class="navbar-item" href="#theory" style="color: #fff; border-bottom: 0px solid #fff;">
        Theoretical Analysis
      </a>
      <a class="navbar-item" href="#BibTeX" style="color: #fff; border-bottom: 0px solid #fff;">
        BibTeX
      </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Training a Generally Curious Agent</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tajwarfahim.github.io/">Fahim Tajwar</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://yidingjiang.github.io/">Yiding Jiang</a><sup>*1</sup>,</span>
              <span class="author-block">
              <a href="https://abitha-thankaraj.github.io/">Abitha Thankaraj</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/sumaitasr/">Sumaita Sadia Rahman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://zicokolter.com/">J Zico Kolter</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~schneide/">Jeff Schneider</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a><sup>1</sup>,
            </span>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
            <span class="author-block"><sup>1</sup>Carnegie Mellon University</span>
            <span class="author-block"><sup>2</sup>North Carolina State University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2502.17543"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.17543"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tajwarfahim/paprika"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>

</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/final_picture_teaser.png"
        alt="Teaser image."
        class="teaser-image"/>
      <h2 class="subtitle has-text-centered">
        Overview of PAPRIKA. We design a diverse set of tasks where an LLM agent needs strategic information gathering to succeed, 
        then train an LLM on self-generated data to prefer higher performing trajectories. The resulting behavior learned by PAPRIKA 
        can transfer zero-shot to unseen tasks, showcasing its potential to build general decision making agents.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="abstract">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Efficient exploration is essential for intelligent systems interacting with their environment, but 
            existing language models often fall short in scenarios that require strategic information gathering. 
            In this paper, we present PAPRIKA, a fine-tuning approach that enables language models to develop general 
            decision-making capabilities that are not confined to particular environments. By training on synthetic 
            interaction data from different tasks that require diverse strategies, PAPRIKA teaches models to explore and 
            adapt their behavior on a new task based on environment feedback in-context without more gradient updates. 
            Experimental results show that models fine-tuned with PAPRIKA can effectively transfer their learned 
            decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, 
            our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. 
            To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories 
            from tasks with high learning potential. These results suggest a promising path towards AI systems that can 
            autonomously solve novel sequential decision-making problems that require interactions with the external world.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  <hr>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h2 class="title is-3" id="setup">Setup</h2>
        <div class="content has-text-justified">
          <p>
            We considered three problems for our experiments: (1) a didactic bandit problems that are easy to simulate, (2) Synthetic LLM 
            fine-tuning problems, where ground-truth reward functions exist and are easy to track (though we still train the policy against a proxy reward model), 
            and (3) Full-scale LLM fine-tuning on AlpacaFarm and UltraFeedback. We use these problems to study the impact of on-policy sampling
            and negative gradients on performance and behavior of algorithms.
          </p>
        </div>
        
        <h3 class="title is-4">Didactic Bandit</h3>
        <div class="content has-text-justified">
          <!-- Create Bullet Point List to Summarize -->
          <ul>
            <li> Simple bandit problem with controllable ground-truth reward function and combinatorial response space </li>
          </ul>
        </div>

        <h3 class="title is-4">Synthetic LLM</h3>
        <div class="content has-text-justified">
          <!-- Create Bullet Point List to Summarize -->
          <ul>
            <li> Used to study the impact of coverage conditions and geometric relationships on different fine-tuning algorithms</li>
            <li> Constructed 3 different scenarios based on the length of the response:</li>
            <ul>
              <li><b>Min length:</b> preferred responses are shorter in length. </li>
              <li><b>Mode length:</b> preferred responses are close to the mode length of responses in the preference dataset.</li>
              <li><b>Skew length:</b> preferred responses are shorter in length, but we modify the preference dataset from Min Length, where dataset completions are truncated to shorter lengths, leading to a skewed length distribution.</li>
            </ul>
          </ul>
        </div>

        <h3 class="title is-4">Full-Scale LLM</h3>
        <div class="content has-text-justified">
          <!-- Create Bullet Point List to Summarize -->
          <ul>
            <li>Used to study the impact of fine-tuning algorithms on realistic LLM fine-tuning data</li>
            <li>AlpacaFarm and UltraFeedback are commonly used domains</li>
          </ul>
        </div>

        <h3 class="title is-4">Coverage Conditions and Geometric Relationships</h3>

        <div class="content has-text-justified">
          <p>
            The performance of preference-finetuning algorithms can be affected by several factors: (1) the coverage of the preference dataset, 
            (2) the reference policy initialization, and (3) the underlying ground truth reward distribution. To study this impact, we consider two
            orthogonal axes: [C1] the geometric alignment between the ground-truth reward function and the reference policy initialization and [C2] the 
            coverage of the preference data used to train the surrogate reward model relative to the reference policy. Condition [C1] is reminiscent of 
            concentrability coefficient in RL and condition [C2] fundamentally imposes a condition on statistical errors in the reward model on different responses. 
          </p>

          <p>
            Understanding the behavior of various approaches as a function of these factors will allow us to better understand the performance of 
            various approaches on downstream fine-tuning in terms of problem geometry [C1] and statistical learning considerations [C2]. More concretely, in the synthetic LLM setting we consider the following cases
          </p>
        </div>
        
        <center>
        <img src="./static/images/coverage.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        
        <div class="content has-text-centered">
          <p>
            We consider three different types of problem scenarios: Min Length, Mode Length, and Skew Length These scenarios are designed to test the
            performance of different fine-tuning algorithms under different conditions.
          </p>
        </div>

        <hr>

        <h2 class="title is-3" id="empirical">Empirical Analysis</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Characterizing Fine-Tuning Methods</h3>
        In this work, we study the role of on-policy sampling and negative gradients in existing preference fine-tuning algorithms. 
        We characterize fine-tuning methods based on three key properties:
        <div class="content has-text-justified">
          <ol>
            <li><b>on-policy sampling:</b> explicit sampling of new responses from the policy or purely learning from offline data</li>
            <li><b>on-policy sample reuse:</b> for only those approaches that perform on-policy sampling, whether the approach makes more 
              than one gradient update on a given prompt-response (ùë•, ùë¶) pair</li>
            <li><b>negative gradient:</b> whether the approach explicitly minimizes a loss that attempts to ‚Äúpush-down‚Äù likelihood on 
              certain responses by multiplying the gradient of their likelihood with a negative coefficient </li>
          </ol>
        </div>
        <center>
        <img src="./static/images/characterize.png"
          alt="On-Policy Sampling."
          width="75%"
          class="on-policy-image"/>
        </center>
        <div class="content has-text-centered">
          We characterize existing fine-tuning methods using these criterion.
        </div>
        <!-- Interpolating. -->
        <h3 class="title is-4">On-Policy Sampling</h3>
        <div class="content has-text-justified">
          <p>
            To study the impact of on-policy sampling, we vary the extent to which updates are made on data from the current policy. 
            Below, we introduce Algorithm 1, which is a encapsulates how fine-tuning algorithms updates on preference data.
          </p>
        </div>
        <center>
        <img src="./static/images/finetuning_algo.png"
          alt="Algorithm 1."
          width="75%"
          class="algo-one-image"/>
        </center>
        <div class="content has-text-centered">
          We characterize existing fine-tuning methods using the following algorithmic formulation.
        </div>

        <div class="content has-text-justified">
          <p>
            One way in which we can control the amount of on-policy sampling in Algorithm 1 is by by varying the total number of samples |ùíü| = ùêµ/ùê∂ √ó ùê∂ = ùêµ 
            used for a given training iteration but keeping minibatch size ùëÄ used for the gradient update fixed, assuming the algorithm performs exactly one pass over all this sampled data. One result from the paper is shown below:
          </p>
        </div>

        <center>
        <img src="./static/images/alpaca_bs.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        <div class="content has-text-centered">
          <p>
            <b>Performance on AlpacaFarm with varying batch sizes.</b> We see that decreasing the batch size improves performance, 
            which leads to more on-policy sampling leads to better performance on the AlpacaFarm domain. Empirically, we see benefits 
            of this sampling when the reference policy is far from the optimal policy.
          </p>
        </div>
        
        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Takeaway: </b>On-policy sampling generally improves performance and efficiency, especially in cases when the 
              peak of reward appears farther from the reference policy, even when the reward model is learned from the same 
              preference dataset that methods use without on-policy learning also use. In some cases, sample reuse can reduce 
              the dependency on on-policy sampling of data, but it presents a tradeoff by reducing exploration of the response space.
            </p>
          </div>
        </div>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Negative Gradients</h3>
        <div class="content has-text-justified">
          <p>
          To understand the role of negative gradient, we will compare contrastive algorithms (e.g., DPO or IPO) with maximum likelihood supervised methods (e.g., Best-of-N) in a fully offline setting, 
          where no new on-policy samples are collected. In the paper, we compare various algorithms and also explore mechanisms behind the behaviors of methods. 

          Our main finding is that negative gradients often speed up convergence of algorithms, often allowing them to reach a better solution. For example, 
          in one of the settings, we compare supervised Best-of-N to supervised Best-of-N with a negative gradient explicitly added and observe that the latter
          is able to quickly learn to find a better solution. Likewise we also find DPO attains better results.
          </p>
        </div>

        <center>
        <img src="./static/images/neg_grad.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        
        <div class="content has-text-centered">
          <p>
            <b>Negative gradients in AlpacaFarm (left) and UltraFeedback (right).</b> For these domains, we consider the increase in average gold reward 
            compared to the SFT model for different offline approaches. Algorithms with a negative gradient such as DPO outperform approaches 
            such as Pref-FT, which do not utilize any negative gradient term in their objective.
          </p>
        </div>

        <center>
        <img src="./static/images/neg_grad_rew.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        <div class="content has-text-centered">
          <p>
            <b>DPO vs Pref-FT Reward Margin.</b> The reward margin between the preferred and dispreferred responses for contrastive algorithms such as 
            DPO is higher than maximum likelihood algorithms such as Pref-FT.
          </p>
        </div>
        
        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Takeaway: </b> A negative gradient improves over offline supervised methods when the peak in the reward appears in less 
              likely regions of the reference policy. It can increase the likelihood of the preferred response, when the dispreferred response is sufficiently 
              different from the prefered response, model capacity is large, and the reference initialization is chosen appropriately. 
              If not, the margin between log likelihoods of preferred and dispreferred will still be larger when a negative gradient is used, 
              but the recovered probability mass will go into increasing likelihoods of other responses, not the preferred response necessarily.
            </p>
          </div>
        </div>

        <h3 class="title is-4">Complementary Nature of On-Policy Sampling and Negative Gradients</h3>
        <div class="content has-text-justified">
          <p>
            On-policy sampling and offline negative gradients present complementary benefits, in that
            the best offline loss function with negative gradients can be used to train on on-policy data,
            improving over on-policy RL or supervised learning. Conceptually, while sampling responses
            on-policy provides coverage of the response space, an effective negative gradient loss provides
            a stronger learning signal given a set of samples. It can also result in computational benefits.
          </p>
        </div>

        <center>
        <img src="./static/images/complimentarity.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        
        <div class="content has-text-centered">
          <p>
            <b>Complementary benefit of on-policy sampling and negative gradients on the synthetic LLM length experiments.</b> 
            Online DPO performs the best where optimal policy and reference policy lies far from each other 
            (min length and skew length), and all algorithms perform similarly when these two policies are close (mode length).
          </p>
        </div>
        <!--/ Re-rendering. -->

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Takeaway: </b>We observe that the best offline loss function with negative gradients can be used to train on on-policy data, improving over on-policy RL or supervised learning. 
              In our experiments, we find this loss function to be a contrastive loss akin to DPO or IPO. Running these contrastive methods on on-policy data helps improve performance. 
              This can also result in computational benefits, improving over wall-clock time.
            </p>
          </div>
        </div>
        
        <hr>
        
        <h2 class="title is-3" id="theory">Theory and Conceptual Unification</h2>
        <div class="content has-text-justified">
          <p>
            With empirical results showing the benefits of on-policy sampling and negative gradient for preference fine-tuning of LLMs, 
            we tie these two approaches together under the notion of mode-seeking objectives in contrast to mode-covering objectives 
            such as (weighted / filtered) maximum likelihood.
          </p>
          <p>
            In the paper, we argue (in Lemmas 6.1 and 6.2) that (1) on-policy algorithms are mode-seeking as they optimize the regularized
            reverse-KL objective and (2) if the negative responses are chosen appropriately, then the contrastive update accelerates the 
            rate of increase of probability mass on the preferred responses and exhibits mode-seeking behavior. For more details, 
            formal definitions, and the proof of these theoretical results, please refer to the paper.
        </div>
        <div class="content has-text-justified">
          <p>
            We then also analyze the benefits of mode-seeking behavior formally via a case study on reverse (mode-seeking) and forward KL (mode-covering) divergences. 
            Prior work typically shows that the benefits of mode-seeking behavior 
            of this sort are more apparent when the model ùëù(ùë•) is unable to realize 
            the target distribution ùëû(ùë•), such that minimizing either KL would give rise to 
            difference solutions. Unlike this prior argument, our theoretical argument in Theorem 6.5
            shows that even when the ùëù(ùë•) can fully represent the target distribution ùëû(ùë•), 
            when training with gradient descent, reverse KL is able to quickly
            re-distribute probability mass to only a subset of the required categories likely in 
            target distribution within a few gradient update steps. 
            This is especially important when early stopping is used and the loss cannot be minimized to exactly 0 
            on the training data -- in such cases, we would expect the reverse KL divergence to quickly redistribute probability mass in a way that is more effective.
            We illustrate a toy version of this idea using a numerical toy experiment as seen below.
          </p>
        </div>
        <center>
        <img src="./static/images/theory_toy.png"
          width="90%"
          alt="On-Policy Sampling."
          class="mode-seeking-image"/>
        </center>
      </div>
    </div>
    <div class="content has-text-centered">
      <p>
        <b>Empirical Example contrasting mode-seeking (Reverse KL) and mode-covering (forward KL) objectives.</b>
      </p>
    </div>
    <div class="box" style="background-color:aliceblue">
      <div class="content has-text-justified">
        <p>
          <b>Takeaway: </b>We conceptually unify on-policy sampling and negative gradients under the notion of mode-seeking objectives, 
          extended to categorical distributions. An analysis of the behavior of some representative mode-seeking and mode-covering objectives
          corroborates our empirical observations in this paper.
        </p>
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>

<hr>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> 
      @misc{tajwar2025traininggenerallycuriousagent,
        title={Training a Generally Curious Agent}, 
        author={Fahim Tajwar and Yiding Jiang and Abitha Thankaraj and Sumaita Sadia Rahman and J Zico Kolter and Jeff Schneider and Ruslan Salakhutdinov},
        year={2025},
        eprint={2502.17543},
        archivePrefix={arXiv},
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2502.17543}, 
      }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2502.17543">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/tajwarfahim/paprika" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <p>
        Corresponding Author: <a href="mailto:ftajwar@cs.cmu.edu">Fahim Tajwar</a>.<br>
        We thank the Nerfies Team for their <a href="https://github.com/nerfies/nerfies.github.io">website template</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
