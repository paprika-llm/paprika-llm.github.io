<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A fine-tuning framework to improve LLMs' strategic exploration and sequential decision making capabilities">
  <meta name="keywords" content="LLM, Synthetic data, Multiturn finetuning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>paprika</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation" style="background-color: #333; color: #fff;">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" style="color: #fff;">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#abstract" style="color: #fff; border-bottom: 0px solid #fff;">
        Abstract
      </a>
      <a class="navbar-item" href="#setup" style="color: #fff; border-bottom: 0px solid #fff;">
        PAPRIKA
      </a>
      <a class="navbar-item" href="#paprika" style="color: #fff; border-bottom: 0px solid #fff;">
        Empirical Analysis
      </a>
      <a class="navbar-item" href="#theory" style="color: #fff; border-bottom: 0px solid #fff;">
        Theoretical Analysis
      </a>
      <a class="navbar-item" href="#BibTeX" style="color: #fff; border-bottom: 0px solid #fff;">
        BibTeX
      </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Training a Generally Curious Agent</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tajwarfahim.github.io/">Fahim Tajwar</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://yidingjiang.github.io/">Yiding Jiang</a><sup>*1</sup>,</span>
              <span class="author-block">
              <a href="https://abitha-thankaraj.github.io/">Abitha Thankaraj</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/sumaitasr/">Sumaita Sadia Rahman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://zicokolter.com/">J Zico Kolter</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~schneide/">Jeff Schneider</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a><sup>1</sup>,
            </span>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
            <span class="author-block"><sup>1</sup>Carnegie Mellon University</span>
            <span class="author-block"><sup>2</sup>North Carolina State University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.17543"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tajwarfahim/paprika"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ftajwar/paprika_SFT_dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>SFT Dataset</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ftajwar/paprika_preference_dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Preference Dataset</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/ftajwar/paprika_Meta-Llama-3.1-8B-Instruct"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-cogs"></i>
                  </span>
                  <span>Model</span>
                  </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>

</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/final_picture_teaser.png"
        alt="Teaser image."
        class="teaser-image"/>
      <h2 class="subtitle has-text-centered">
        Overview of <b>PAPRIKA</b>. We design a diverse set of tasks where an LLM agent needs strategic information gathering to succeed, 
        then train an LLM on self-generated data to prefer higher performing trajectories. The resulting behavior learned by PAPRIKA 
        can transfer zero-shot to unseen tasks, showcasing its potential to build general decision making agents.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="abstract">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Efficient exploration is essential for intelligent systems interacting with their environment, but 
            existing language models often fall short in scenarios that require strategic information gathering. 
            In this paper, we present <b>PAPRIKA</b>, a fine-tuning approach that enables language models to develop general 
            decision-making capabilities that are not confined to particular environments. By training on synthetic 
            interaction data from different tasks that require diverse strategies, <b>PAPRIKA</b> teaches models to explore and 
            adapt their behavior on a new task based on environment feedback in-context without more gradient updates. 
            Experimental results show that models fine-tuned with <b>PAPRIKA</b> can effectively transfer their learned 
            decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, 
            our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. 
            To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories 
            from tasks with high learning potential. These results suggest a promising path towards AI systems that can 
            autonomously solve novel sequential decision-making problems that require interactions with the external world.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  <hr>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h2 class="title is-3" id="setup">Problem Setting</h2>
        <div class="content has-text-justified">
          <p>
            The goal of our paper is to develop a scalable method to instill better strategic exploration and sequential decision-making capabilities into LLMs. 
            Prior works (<a href="https://arxiv.org/abs/2310.07064">Krishnamurthy et al., 2024</a>) have shown that LLMs can perform poorly on even the simple decision making task of multi-armed bandits. 
            <a href="https://arxiv.org/abs/2410.06238">Nie et al., 2024</a> has since then demonstrated that LLMs can be taught to perform better on bandits after fine-tuning them on synthetic trajectories 
            generated by known algorithms such as UCB. However, this idea is limited in scope for three reasons:
          </p>
          <ul>
            <li><b>(1)</b> We want LLMs to perform strategic exploration and decision making in more complex settings</li>
            <li><b>(2)</b> For most tasks, there is no known algorithm like UCB to generate good synthetic trajectories from</li>
            <li><b>(3)</b> It can be infeasible to collect data for all tasks that we care about</li>
          </ul>
        </div>

        <h2 class="title is-3" id="paprika">PAPRIKA</h2>
        <div class="content has-text-justified">
          PAPRIKA aims to solve the above problems. First, we design a suite of complex decision-making tasks that require strategic information gathering to succeed. Next, we show that in the absence of known good algorithms, existing LLMs can generate trajectories with better decision making behaviors through diversity-encouraging sampling. We then finetune the LLMs to prefer higher performing trajectories (in a fashion similar to <a href="https://arxiv.org/abs/2203.14465">STaR</a>) and show that this leads to better decision making abilities at test-time. More importantly, <b>these behaviors often generalize to unseen task groups without additional training</b>. Finally, we propose a general curriculum learning algorithm that can dynamically choose which subset of tasks to train on next to improve data efficiency of such training methods. We next describe each component of PAPRIKA.
        </div>
        
        <h3 class="title is-4">Task Design</h3>
        <div class="content has-text-justified">
          <!-- Create Bullet Point List to Summarize -->
          To both evaluate and then train LLMs, we design 10 diverse textual task groups, each of which comprises of partially observable tasks that require multiturn interaction with the task environment, strategic exploration and good sequential decision making abilities for an agent to succeed at them. Below is a summary of these task groups.
        </div>

        <center>
          <img src="./static/images/paprika_task_groups.png"
            width="90%"
            alt="paprika_task_groups"
            class="paprika_task_groups"/>
        </center>

        <h3 class="title is-4">Optimization</h3>
        <div class="content has-text-justified">
          <!-- Create Bullet Point List to Summarize -->
          We use a multiturn variant of supervised fine-tuning (SFT) and Direct Preference Optimization (<a href="https://arxiv.org/abs/2305.18290">DPO</a>) to fine-tune our models, where log probabilities are calculated autoregressively over the entire trajectory, but then
          only the log probabilities of the agent generated tokens are considered for calculating the training loss. In practice, we first run supervised finetuning, and then use the sum of SFT and DPO (similar to <a href="https://arxiv.org/abs/2404.19733">RPO</a>) losses to optimize our LLMs.
        </div>

        <center>
          <img src="./static/images/optimization_objective.png"
            width="40%"
            alt="optimization_objective"
            class="optimization_objective"/>
        </center>

        <h3 class="title is-4">Scalable Online Curriculum Learning Algorithm</h3>
        <div class="content has-text-justified">
          PAPRIKA's primary bottleneck lies in generating training trajectories, rather than model updates. So it is crucial that we generate more rollouts on tasks that have high learning potential. 
        </div>

        <div class="content has-text-justified">
          However, it is hard to know which tasks would have high learning potential without generating rollouts first. We make the additional assumption that similar tasks have similar learning potential, and that we have task similarity groups over our set of tasks. Under this assumption, we can sample a few tasks from a task group, generate multiple trajectories per task to calculate the coefficient of variation (over number of turns, which we use as a proxy for reward). We use the coefficient of variation as a metric for measuring learning potential.
        </div>

        <center>
          <img src="./static/images/measuring_learning_potential.png"
            width="70%"
            alt="learning_potential"
            class="learning_potential"/>
        </center>

        <div class="content has-text-justified">
          Once we have metric for measuring learning potential, we naturally want to maximize the learning potential of our sampled dataset, and we can treat this as multi-armed bandit (MAB) problem, for which we can employ the Upper Confidence Bound (UCB) algorithm to sample tasks from the collection of task groups. 
        </div>

        <center>
          <img src="./static/images/task_selection_with_UCB.png"
            width="70%"
            alt="curriculum_algorithm"
            class="curriculum_algorithm"/>
        </center>

        <h3 class="title is-4">Coverage Conditions and Geometric Relationships</h3>

        <div class="content has-text-justified">
          <p>
            The performance of preference-finetuning algorithms can be affected by several factors: (1) the coverage of the preference dataset, 
            (2) the reference policy initialization, and (3) the underlying ground truth reward distribution. To study this impact, we consider two
            orthogonal axes: [C1] the geometric alignment between the ground-truth reward function and the reference policy initialization and [C2] the 
            coverage of the preference data used to train the surrogate reward model relative to the reference policy. Condition [C1] is reminiscent of 
            concentrability coefficient in RL and condition [C2] fundamentally imposes a condition on statistical errors in the reward model on different responses. 
          </p>

          <p>
            Understanding the behavior of various approaches as a function of these factors will allow us to better understand the performance of 
            various approaches on downstream fine-tuning in terms of problem geometry [C1] and statistical learning considerations [C2]. More concretely, in the synthetic LLM setting we consider the following cases
          </p>
        </div>
        
        <center>
        <img src="./static/images/coverage.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        
        <div class="content has-text-centered">
          <p>
            We consider three different types of problem scenarios: Min Length, Mode Length, and Skew Length These scenarios are designed to test the
            performance of different fine-tuning algorithms under different conditions.
          </p>
        </div>

        <hr>

        <h2 class="title is-3" id="empirical">Empirical Analysis</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Characterizing Fine-Tuning Methods</h3>
        In this work, we study the role of on-policy sampling and negative gradients in existing preference fine-tuning algorithms. 
        We characterize fine-tuning methods based on three key properties:
        <div class="content has-text-justified">
          <ol>
            <li><b>on-policy sampling:</b> explicit sampling of new responses from the policy or purely learning from offline data</li>
            <li><b>on-policy sample reuse:</b> for only those approaches that perform on-policy sampling, whether the approach makes more 
              than one gradient update on a given prompt-response (ùë•, ùë¶) pair</li>
            <li><b>negative gradient:</b> whether the approach explicitly minimizes a loss that attempts to ‚Äúpush-down‚Äù likelihood on 
              certain responses by multiplying the gradient of their likelihood with a negative coefficient </li>
          </ol>
        </div>
        <center>
        <img src="./static/images/characterize.png"
          alt="On-Policy Sampling."
          width="75%"
          class="on-policy-image"/>
        </center>
        <div class="content has-text-centered">
          We characterize existing fine-tuning methods using these criterion.
        </div>
        <!-- Interpolating. -->
        <h3 class="title is-4">On-Policy Sampling</h3>
        <div class="content has-text-justified">
          <p>
            To study the impact of on-policy sampling, we vary the extent to which updates are made on data from the current policy. 
            Below, we introduce Algorithm 1, which is a encapsulates how fine-tuning algorithms updates on preference data.
          </p>
        </div>
        <center>
        <img src="./static/images/finetuning_algo.png"
          alt="Algorithm 1."
          width="75%"
          class="algo-one-image"/>
        </center>
        <div class="content has-text-centered">
          We characterize existing fine-tuning methods using the following algorithmic formulation.
        </div>

        <div class="content has-text-justified">
          <p>
            One way in which we can control the amount of on-policy sampling in Algorithm 1 is by by varying the total number of samples |ùíü| = ùêµ/ùê∂ √ó ùê∂ = ùêµ 
            used for a given training iteration but keeping minibatch size ùëÄ used for the gradient update fixed, assuming the algorithm performs exactly one pass over all this sampled data. One result from the paper is shown below:
          </p>
        </div>

        <center>
        <img src="./static/images/alpaca_bs.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        <div class="content has-text-centered">
          <p>
            <b>Performance on AlpacaFarm with varying batch sizes.</b> We see that decreasing the batch size improves performance, 
            which leads to more on-policy sampling leads to better performance on the AlpacaFarm domain. Empirically, we see benefits 
            of this sampling when the reference policy is far from the optimal policy.
          </p>
        </div>
        
        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Takeaway: </b>On-policy sampling generally improves performance and efficiency, especially in cases when the 
              peak of reward appears farther from the reference policy, even when the reward model is learned from the same 
              preference dataset that methods use without on-policy learning also use. In some cases, sample reuse can reduce 
              the dependency on on-policy sampling of data, but it presents a tradeoff by reducing exploration of the response space.
            </p>
          </div>
        </div>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Negative Gradients</h3>
        <div class="content has-text-justified">
          <p>
          To understand the role of negative gradient, we will compare contrastive algorithms (e.g., DPO or IPO) with maximum likelihood supervised methods (e.g., Best-of-N) in a fully offline setting, 
          where no new on-policy samples are collected. In the paper, we compare various algorithms and also explore mechanisms behind the behaviors of methods. 

          Our main finding is that negative gradients often speed up convergence of algorithms, often allowing them to reach a better solution. For example, 
          in one of the settings, we compare supervised Best-of-N to supervised Best-of-N with a negative gradient explicitly added and observe that the latter
          is able to quickly learn to find a better solution. Likewise we also find DPO attains better results.
          </p>
        </div>

        <center>
        <img src="./static/images/neg_grad.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        
        <div class="content has-text-centered">
          <p>
            <b>Negative gradients in AlpacaFarm (left) and UltraFeedback (right).</b> For these domains, we consider the increase in average gold reward 
            compared to the SFT model for different offline approaches. Algorithms with a negative gradient such as DPO outperform approaches 
            such as Pref-FT, which do not utilize any negative gradient term in their objective.
          </p>
        </div>

        <center>
        <img src="./static/images/neg_grad_rew.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        <div class="content has-text-centered">
          <p>
            <b>DPO vs Pref-FT Reward Margin.</b> The reward margin between the preferred and dispreferred responses for contrastive algorithms such as 
            DPO is higher than maximum likelihood algorithms such as Pref-FT.
          </p>
        </div>
        
        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Takeaway: </b> A negative gradient improves over offline supervised methods when the peak in the reward appears in less 
              likely regions of the reference policy. It can increase the likelihood of the preferred response, when the dispreferred response is sufficiently 
              different from the prefered response, model capacity is large, and the reference initialization is chosen appropriately. 
              If not, the margin between log likelihoods of preferred and dispreferred will still be larger when a negative gradient is used, 
              but the recovered probability mass will go into increasing likelihoods of other responses, not the preferred response necessarily.
            </p>
          </div>
        </div>

        <h3 class="title is-4">Complementary Nature of On-Policy Sampling and Negative Gradients</h3>
        <div class="content has-text-justified">
          <p>
            On-policy sampling and offline negative gradients present complementary benefits, in that
            the best offline loss function with negative gradients can be used to train on on-policy data,
            improving over on-policy RL or supervised learning. Conceptually, while sampling responses
            on-policy provides coverage of the response space, an effective negative gradient loss provides
            a stronger learning signal given a set of samples. It can also result in computational benefits.
          </p>
        </div>

        <center>
        <img src="./static/images/complimentarity.png"
          width="90%"
          alt="On-Policy Sampling."
          class="on-policy-image"/>
        </center>
        
        <div class="content has-text-centered">
          <p>
            <b>Complementary benefit of on-policy sampling and negative gradients on the synthetic LLM length experiments.</b> 
            Online DPO performs the best where optimal policy and reference policy lies far from each other 
            (min length and skew length), and all algorithms perform similarly when these two policies are close (mode length).
          </p>
        </div>
        <!--/ Re-rendering. -->

        <div class="box" style="background-color:aliceblue">
          <div class="content has-text-justified">
            <p>
              <b>Takeaway: </b>We observe that the best offline loss function with negative gradients can be used to train on on-policy data, improving over on-policy RL or supervised learning. 
              In our experiments, we find this loss function to be a contrastive loss akin to DPO or IPO. Running these contrastive methods on on-policy data helps improve performance. 
              This can also result in computational benefits, improving over wall-clock time.
            </p>
          </div>
        </div>
        
        <hr>
        
        <h2 class="title is-3" id="theory">Theory and Conceptual Unification</h2>
        <div class="content has-text-justified">
          <p>
            With empirical results showing the benefits of on-policy sampling and negative gradient for preference fine-tuning of LLMs, 
            we tie these two approaches together under the notion of mode-seeking objectives in contrast to mode-covering objectives 
            such as (weighted / filtered) maximum likelihood.
          </p>
          <p>
            In the paper, we argue (in Lemmas 6.1 and 6.2) that (1) on-policy algorithms are mode-seeking as they optimize the regularized
            reverse-KL objective and (2) if the negative responses are chosen appropriately, then the contrastive update accelerates the 
            rate of increase of probability mass on the preferred responses and exhibits mode-seeking behavior. For more details, 
            formal definitions, and the proof of these theoretical results, please refer to the paper.
        </div>
        <div class="content has-text-justified">
          <p>
            We then also analyze the benefits of mode-seeking behavior formally via a case study on reverse (mode-seeking) and forward KL (mode-covering) divergences. 
            Prior work typically shows that the benefits of mode-seeking behavior 
            of this sort are more apparent when the model ùëù(ùë•) is unable to realize 
            the target distribution ùëû(ùë•), such that minimizing either KL would give rise to 
            difference solutions. Unlike this prior argument, our theoretical argument in Theorem 6.5
            shows that even when the ùëù(ùë•) can fully represent the target distribution ùëû(ùë•), 
            when training with gradient descent, reverse KL is able to quickly
            re-distribute probability mass to only a subset of the required categories likely in 
            target distribution within a few gradient update steps. 
            This is especially important when early stopping is used and the loss cannot be minimized to exactly 0 
            on the training data -- in such cases, we would expect the reverse KL divergence to quickly redistribute probability mass in a way that is more effective.
            We illustrate a toy version of this idea using a numerical toy experiment as seen below.
          </p>
        </div>
        <center>
        <img src="./static/images/theory_toy.png"
          width="90%"
          alt="On-Policy Sampling."
          class="mode-seeking-image"/>
        </center>
      </div>
    </div>
    <div class="content has-text-centered">
      <p>
        <b>Empirical Example contrasting mode-seeking (Reverse KL) and mode-covering (forward KL) objectives.</b>
      </p>
    </div>
    <div class="box" style="background-color:aliceblue">
      <div class="content has-text-justified">
        <p>
          <b>Takeaway: </b>We conceptually unify on-policy sampling and negative gradients under the notion of mode-seeking objectives, 
          extended to categorical distributions. An analysis of the behavior of some representative mode-seeking and mode-covering objectives
          corroborates our empirical observations in this paper.
        </p>
      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>

<hr>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> 
      @misc{tajwar2025traininggenerallycuriousagent,
        title={Training a Generally Curious Agent}, 
        author={Fahim Tajwar and Yiding Jiang and Abitha Thankaraj and Sumaita Sadia Rahman and J Zico Kolter and Jeff Schneider and Ruslan Salakhutdinov},
        year={2025},
        eprint={2502.17543},
        archivePrefix={arXiv},
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2502.17543}, 
      }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2502.17543">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/tajwarfahim/paprika" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <p>
        Corresponding Author: <a href="mailto:ftajwar@cs.cmu.edu">Fahim Tajwar</a>.<br>
        We thank the Nerfies Team for their <a href="https://github.com/nerfies/nerfies.github.io">website template</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
